#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€å•ç”Ÿæˆè„šæœ¬ - ä»å¤´ç”ŸæˆéŸ³ä¹ï¼ˆæ— æ¡ä»¶ç”Ÿæˆï¼‰
"""

import json
import torch
from transformers import GPT2LMHeadModel, AutoTokenizer


def generate_music_from_scratch():
    """ä»å¤´ç”ŸæˆéŸ³ä¹"""
    
    # ==================== é…ç½® ====================
    MODEL_PATH = "./model_output/checkpoint-6000"  # ä½¿ç”¨æœ€ä½³ checkpoint
    OUTPUT_FILE = "./generated_from_scratch.json"
    
    # ç”Ÿæˆå‚æ•°
    NUM_SEQUENCES = 3          # ç”Ÿæˆ 3 é¦–æ›²å­
    MAX_LENGTH = 600           # æ¯é¦–æ›²å­çš„é•¿åº¦
    TEMPERATURE = 0.9          # æ¸©åº¦ï¼ˆ0.8-0.9 ä¿å®ˆï¼Œ1.0-1.1 åˆ›é€ æ€§ï¼‰
    TOP_K = 50
    TOP_P = 0.95
    
    # ==================== åŠ è½½æ¨¡å‹ ====================
    TOKENIZER_PATH = "./tokenizer"
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_PATH}")
    print(f"æ­£åœ¨åŠ è½½ tokenizer: {TOKENIZER_PATH}")
    
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)
    model = GPT2LMHeadModel.from_pretrained(MODEL_PATH)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    
    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ ({device})")
    
    # ==================== ç”ŸæˆéŸ³ä¹ ====================
    print(f"\n" + "="*60)
    print(f"å¼€å§‹ç”ŸæˆéŸ³ä¹...")
    print(f"  - ç”Ÿæˆæ•°é‡: {NUM_SEQUENCES}")
    print(f"  - æ¯é¦–é•¿åº¦: {MAX_LENGTH} tokens")
    print(f"  - Temperature: {TEMPERATURE}")
    print("="*60 + "\n")
    
    # ä» BOS token å¼€å§‹ç”Ÿæˆ
    input_ids = torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long).to(device)
    
    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_length=MAX_LENGTH,
            temperature=TEMPERATURE,
            top_k=TOP_K,
            top_p=TOP_P,
            num_return_sequences=NUM_SEQUENCES,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            do_sample=True,
        )
    
    # è½¬æ¢ç»“æœ
    generated_sequences = []
    for i, output in enumerate(output_ids):
        tokens = output.cpu().tolist()
        
        # ç§»é™¤ç‰¹æ®Š token
        filtered_tokens = [t for t in tokens if t not in [
            tokenizer.pad_token_id,
            tokenizer.bos_token_id,
            tokenizer.eos_token_id,
        ]]
        
        generated_sequences.append(filtered_tokens)
        print(f"  âœ“ ç”Ÿæˆç¬¬ {i+1} é¦–: {len(filtered_tokens)} tokens")
    
    # ==================== ä¿å­˜ç»“æœ ====================
    output_data = {
        "model_checkpoint": MODEL_PATH,
        "generation_type": "unconditional (from scratch)",
        "generation_params": {
            "max_length": MAX_LENGTH,
            "temperature": TEMPERATURE,
            "top_k": TOP_K,
            "top_p": TOP_P,
        },
        "num_sequences": len(generated_sequences),
        "sequences": generated_sequences,
    }
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"\n" + "="*60)
    print(f"âœ“ ç”Ÿæˆå®Œæˆï¼")
    print(f"  - ç»“æœä¿å­˜åœ¨: {OUTPUT_FILE}")
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"  1. ç”¨ä½ çš„è§£ç å™¨å°† token IDs è½¬æ¢å› MIDI")
    print(f"  2. æ’­æ”¾ MIDI æ–‡ä»¶å¬å¬æ•ˆæœ")
    print(f"  3. å¦‚æœä¸æ»¡æ„ï¼Œå¯ä»¥è°ƒæ•´ temperature é‡æ–°ç”Ÿæˆ")
    print("="*60)


if __name__ == "__main__":
    generate_music_from_scratch()
